{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Soft Actor Critical Agent\n",
    "\n",
    "Use some image transformations :\n",
    "- crop (1/3 of height)\n",
    "- sobel_edges\n",
    "- auto encoder (variational auto encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import tempfile\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "\n",
    "from xebikart.gym.envs.donkey_env import DonkeyEnv\n",
    "from xebikart.gym.envs.wrappers import CropObservationWrapper, ConvVariationalAutoEncoderObservationWrapper, \\\n",
    "    HistoryBasedWrapper, EdgingObservationWrapper\n",
    "\n",
    "from xebikart.rl.sac import CustomSAC, CustomSACPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_start = 300\n",
    "steps = 1000\n",
    "vae_mlflow_runid = \"576b04c6640d4d7c834a928b6bb4fb9a\"\n",
    "sac_policy = CustomSACPolicy\n",
    "level = 0\n",
    "use_vae = True\n",
    "use_history = False\n",
    "n_history = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory\n",
    "tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Donkey Env\n",
    "\n",
    "Start a donkey simulator in level 0.\n",
    "\n",
    "Add wrappers (Crop -> VAE -> History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create donkey env\n",
    "donkey_env = DonkeyEnv(\n",
    "  level=level, frame_skip=2, max_cte_error=3.0, \n",
    "  min_steering=-1, max_steering=1,\n",
    "  min_throttle=0.4, max_throttle=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "vae = mlflow.keras.load_model(f\"runs:/{vae_mlflow_runid}/model\", compile=False)\n",
    "# CropObservation\n",
    "crop_obs = CropObservationWrapper(donkey_env, 0, 40, 160, 80)\n",
    "# Edging\n",
    "edging_obs = EdgingObservationWrapper(crop_obs)\n",
    "# VAE\n",
    "vae_obs = ConvVariationalAutoEncoderObservationWrapper(edging_obs, vae)\n",
    "# History\n",
    "history_obs = HistoryBasedWrapper(vae_obs, n_command_history=n_history, max_steering_diff=0.15, jerk_penalty_weight=0.)\n",
    "\n",
    "use_obs = edging_obs\n",
    "if use_vae:\n",
    "    use_obs = history_obs if use_history else vae_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set monitor\n",
    "log_dir = os.path.join(tempdir, \"log_dir\")\n",
    "donkey_env_monitored = Monitor(use_obs, log_dir, allow_early_resets=True)\n",
    "# Create DummyVecEnv\n",
    "env = DummyVecEnv([lambda: donkey_env_monitored])\n",
    "# Record the video starting at the first step\n",
    "videos_path = os.path.join(tempdir, \"videos\")\n",
    "env = VecVideoRecorder(env, videos_path,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- param policy: (SACPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, LnMlpPolicy, ...)\n",
    "- param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "- param gamma: (float) the discount factor\n",
    "- param learning_rate: (float or callable) learning rate for adam optimizer,\n",
    "    the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n",
    "    it can be a function of the current progress (from 1 to 0)\n",
    "- param buffer_size: (int) size of the replay buffer\n",
    "- param batch_size: (int) Minibatch size for each gradient update\n",
    "- param tau: (float) the soft update coefficient (\"polyak update\", between 0 and 1)\n",
    "- param ent_coef: (str or float) Entropy regularization coefficient. (Equivalent to\n",
    "    inverse of reward scale in the original SAC paper.)  Controlling exploration/exploitation trade-off.\n",
    "    Set it to 'auto' to learn it automatically (and 'auto_0.1' for using 0.1 as initial value)\n",
    "- param train_freq: (int) Update the model every `train_freq` steps.\n",
    "- param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n",
    "- param target_update_interval: (int) update the target network every `target_network_update_freq` steps.\n",
    "- param gradient_steps: (int) How many gradient update after each step\n",
    "- param target_entropy: (str or float) target entropy when learning ent_coef (ent_coef = 'auto')\n",
    "- param action_noise: (ActionNoise) the action noise type (None by default), this can help\n",
    "    for hard exploration problem. Cf DDPG for the different action noise type.\n",
    "- param random_exploration: (float) Probability of taking a random action (as in an epsilon-greedy strategy)\n",
    "    This is not needed for SAC normally but can help exploring when using HER + SAC.\n",
    "    This hack was present in the original OpenAI Baselines repo (DDPG + HER)\n",
    "- param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "- param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "- param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "- param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n",
    "- param full_tensorboard_log: (bool) enable additional logging when using tensorboard\n",
    "    Note: this has no effect on SAC logging for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and create log path\n",
    "tensorboard_log = os.path.join(tempdir, \"tensorboard\")\n",
    "\n",
    "model = CustomSAC(env=env, policy=sac_policy, \n",
    "    learning_rate=3e-4, train_freq=300, buffer_size=30000, \n",
    "    batch_size=64, gamma=0.99, gradient_steps=600,\n",
    "    learning_starts=learning_start, ent_coef=\"auto_0.1\",\n",
    "    tensorboard_log=tensorboard_log, verbose=1, full_tensorboard_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an agent from scratch\n",
    "mlflow.set_experiment(\"soft_actor_critical_edges\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"sac_policy\": sac_policy,\n",
    "        \"donkey_car_level\": level,\n",
    "        \"use_vae\": use_vae,\n",
    "        \"vae_run_id\": vae_mlflow_runid if use_vae else None,\n",
    "        \"use_history\": use_history,\n",
    "        \"n_history\": n_history if use_history else None,\n",
    "        \"steps\": steps\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"steps\" : steps\n",
    "    }, step =10)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    # Save trained model\n",
    "    model.save(os.path.join(tempdir, \"model\"))\n",
    "    mlflow.log_artifacts(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donkey_env.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlflow.tracking.artifact_utils import _download_artifact_from_uri\n",
    "\n",
    "#run_id = \"f288e78b08374a6a90078fe08ae5b9cc\"\n",
    "#local_path_uri = _download_artifact_from_uri(f\"runs:/{run_id}/model.pkl\")\n",
    "#sac = CustomSAC.load(local_path_uri)\n",
    "# gradient_steps not saved\n",
    "#sac.gradient_steps = 600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
