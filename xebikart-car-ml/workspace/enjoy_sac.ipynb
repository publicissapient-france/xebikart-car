{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines.common import set_global_seeds\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from config import ENV_ID\n",
    "\n",
    "from vae.controller import VAEController\n",
    "from donkey_gym.envs.donkey_env import DonkeyVAEHistoryEnv\n",
    "\n",
    "from algos.custom_sac import SACWithVAE, CustomSACPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(episode, step, obs, reward, info):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(obs)\n",
    "    plt.title(\"Episode : %d.%d | Reward: %f | Info: %s\" % (episode, step, reward, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "vae = VAEController()\n",
    "vae.load(\"vae-level-0-dim-32.pkl\")\n",
    "\n",
    "env = DonkeyVAEHistoryEnv(\n",
    "  level=0, frame_skip=2, max_cte_error=3.0, \n",
    "  vae=vae, n_command_history=20, \n",
    "  min_steering=-1, max_steering=1,\n",
    "  min_throttle=0.4, max_throttle=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SACWithVAE.load(\"logs/sac/0/model.pkl\")\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Force deterministic for SAC and DDPG\n",
    "deterministic = True\n",
    "\n",
    "print(\"Deterministic actions: {}\".format(deterministic))\n",
    "\n",
    "running_reward = 0.0\n",
    "ep_len = 0\n",
    "while True:\n",
    "    action, _ = model.predict(obs, deterministic=deterministic)\n",
    "    # Clip Action to avoid out of bound errors\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "    obs, reward, done, infos = env.step(action)\n",
    "    # Show env\n",
    "    show_state(0, 0, env.render('rgb_array'), reward, infos)\n",
    "    running_reward += reward\n",
    "    ep_len += 1\n",
    "\n",
    "    if done:\n",
    "        # NOTE: for env using VecNormalize, the mean reward\n",
    "        # is a normalized reward when `--norm_reward` flag is passed\n",
    "        print(\"Episode Reward: {:.2f}\".format(running_reward))\n",
    "        print(\"Episode Length\", ep_len)\n",
    "        running_reward = 0.0\n",
    "        ep_len = 0\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
