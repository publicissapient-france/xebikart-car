{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import signal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.transform import rotate\n",
    "from skimage.viewer import ImageViewer\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import normal, identity\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from donkey_gym.envs.donkey_env import DonkeyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils Functions ##\n",
    "\n",
    "def linear_bin(a):\n",
    "    \"\"\"\n",
    "    Convert a value to a categorical array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int or float\n",
    "        A value between -1 and 1\n",
    "    Returns\n",
    "    -------\n",
    "    list of int\n",
    "        A list of length 15 with one item set to 1, which represents the linear value, and all other items set to 0.\n",
    "    \"\"\"\n",
    "    a = a + 1\n",
    "    b = round(a / (2 / 14))\n",
    "    arr = np.zeros(15)\n",
    "    arr[int(b)] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def linear_unbin(arr):\n",
    "    \"\"\"\n",
    "    Convert a categorical array to value.\n",
    "    See Also\n",
    "    --------\n",
    "    linear_bin\n",
    "    \"\"\"\n",
    "    if not len(arr) == 15:\n",
    "        raise ValueError('Illegal array length, must be 15')\n",
    "    b = np.argmax(arr)\n",
    "    a = b * (2 / 14) - 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(episode, step, obs, reward, info):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(obs)\n",
    "    plt.title(\"Episode : %d.%d | Reward: %f | Info: %s\" % (episode, step, reward, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_space, train=True):\n",
    "        self.t = 0\n",
    "        self.max_Q = 0\n",
    "        self.train = train\n",
    "        \n",
    "        # Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_space = action_space\n",
    "        self.action_size = action_space\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        if (self.train):\n",
    "            self.epsilon = 1.0\n",
    "            self.initial_epsilon = 1.0\n",
    "        else:\n",
    "            self.epsilon = 1e-6\n",
    "            self.initial_epsilon = 1e-6\n",
    "        self.epsilon_min = 0.02\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 100\n",
    "        self.explore = 10000\n",
    "\n",
    "        # Create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # Create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # Copy the model to target model\n",
    "        # --> initialize the target model so that the parameters of model & target model to be same\n",
    "        self.update_target_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(24, (5, 5), strides=(2, 2), padding=\"same\",input_shape=self.state_size))  #80*80*4\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # 15 categorical bins for Steering angles\n",
    "        model.add(Dense(15, activation=\"linear\")) \n",
    "\n",
    "        adam = Adam(lr=self.learning_rate)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def rgb2gray(self, rgb):\n",
    "        '''\n",
    "        take a numpy rgb image return a new single channel image converted to greyscale\n",
    "        '''\n",
    "        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "\n",
    "    def process_image(self, obs):\n",
    "        obs = self.rgb2gray(obs)\n",
    "        obs = cv2.resize(obs, (self.state_size[0], self.state_size[1]))\n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, s_t):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.action_space.sample()[0]       \n",
    "        else:\n",
    "            #print(\"Return Max Q Prediction\")\n",
    "            q_value = self.model.predict(s_t)\n",
    "\n",
    "            # Convert q array to steering value\n",
    "            return linear_unbin(q_value[0])\n",
    "\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= (self.initial_epsilon - self.epsilon_min) / self.explore\n",
    "\n",
    "\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
    "        state_t = np.concatenate(state_t)\n",
    "        state_t1 = np.concatenate(state_t1)\n",
    "        targets = self.model.predict(state_t)\n",
    "        self.max_Q = np.max(targets[0])\n",
    "        target_val = self.model.predict(state_t1)\n",
    "        target_val_ = self.target_model.predict(state_t1)\n",
    "        for i in range(batch_size):\n",
    "            if terminal[i]:\n",
    "                targets[i][action_t[i]] = reward_t[i]\n",
    "            else:\n",
    "                a = np.argmax(target_val[i])\n",
    "                targets[i][action_t[i]] = reward_t[i] + self.discount_factor * (target_val_[i][a])\n",
    "\n",
    "        self.model.train_on_batch(state_t, targets)\n",
    "\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "\n",
    "    # Save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn(env, agent, nb_episodes=100, throttle=0.3):\n",
    "    '''\n",
    "    run a DDQN training session, or test it's result, with the donkey simulator\n",
    "    '''\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "\n",
    "    #not working on windows...\n",
    "    def signal_handler(signal, frame):\n",
    "        print(\"catching ctrl+c\")\n",
    "        env.unwrapped.close()\n",
    "        sys.exit(0)\n",
    "\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "    signal.signal(signal.SIGABRT, signal_handler)\n",
    "\n",
    "    try:\n",
    "        throttle = throttle # Set throttle as constant value\n",
    "\n",
    "        episodes = []\n",
    "\n",
    "        for e in range(nb_episodes):\n",
    "\n",
    "            print(\"Episode: \", e)\n",
    "\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "\n",
    "            episode_len = 0\n",
    "        \n",
    "            x_t = agent.process_image(obs)\n",
    "\n",
    "            s_t = np.stack((x_t,x_t,x_t,x_t),axis=2)\n",
    "            # In Keras, need to reshape\n",
    "            s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2]) #1*80*80*4       \n",
    "            \n",
    "            while not done:\n",
    "                # Get action for the current state and go one step in environment\n",
    "                steering = agent.get_action(s_t)\n",
    "                action = [steering, throttle]\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Show env\n",
    "                show_state(e, episode_len, next_obs, reward, info)\n",
    "\n",
    "                x_t1 = agent.process_image(next_obs)\n",
    "\n",
    "                x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "                s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) #1x80x80x4\n",
    "\n",
    "                # Save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.replay_memory(s_t, np.argmax(linear_bin(steering)), reward, s_t1, done)\n",
    "                agent.update_epsilon()\n",
    "\n",
    "                agent.train_replay()\n",
    "\n",
    "                s_t = s_t1\n",
    "                agent.t = agent.t + 1\n",
    "                episode_len = episode_len + 1\n",
    "                #if agent.t % 30 == 0:\n",
    "                #    print(\"EPISODE\",  e, \"TIMESTEP\", agent.t,\"/ ACTION\", action, \"/ REWARD\", reward, \"/ EPISODE LENGTH\", episode_len, \"/ Q_MAX \" , agent.max_Q)\n",
    "\n",
    "                if done:\n",
    "\n",
    "                    # Every episode update the target model to be same with model\n",
    "                    agent.update_target_model()\n",
    "\n",
    "                    episodes.append(e)\n",
    "                    \n",
    "                    agent.save_model(\"tmp.model\")\n",
    "\n",
    "                    print(\"episode:\", e, \"  memory length:\", len(agent.memory),\n",
    "                        \"  epsilon:\", agent.epsilon, \" episode length:\", episode_len)\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopping run...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DonkeyEnv(\n",
    "  level=0, frame_skip=2, max_cte_error=3.0, \n",
    "  min_steering=-1, max_steering=1,\n",
    "  min_throttle=0.3, max_throttle=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space.shape, env.action_space, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using agent\n",
    "train_ddqn(env, agent, nb_episodes=100, throttle=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for each episode\n",
    "agent.save_model(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
