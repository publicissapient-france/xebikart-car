{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "Train a soft actor critic agent based on rl_coach framework : https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html\n",
    "\n",
    "Some \"filters\" are applied on observation before being supply to soft actor critic policies :\n",
    "- Convert tensor uint8 type into float32\n",
    "- Convert rgb images to grayscale\n",
    "- Reshape image by cropping from (120, 160) -> (80, 160)\n",
    "- Apply sobel filter (https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "  - Binarize images by setting elements to 0 or 1\n",
    "- Embed image with variational auto encoder\n",
    "- Concat last X actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "from rl_coach.agents.soft_actor_critic_agent import SoftActorCriticAgentParameters\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import SimpleSchedule\n",
    "from rl_coach.base_parameters import TaskParameters, VisualizationParameters, PresetValidationParameters\n",
    "from rl_coach.core_types import EnvironmentSteps, TrainingSteps, SelectedPhaseOnlyDumpFilter, RunPhase\n",
    "from rl_coach.schedules import LinearSchedule\n",
    "\n",
    "from rl_coach import logger\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_history = 30\n",
    "improve_steps = 20000\n",
    "heatup_steps = 300\n",
    "evaluation_steps = 800\n",
    "steps_between_evaluation_periods = 10000\n",
    "num_training_per_episode = 1000\n",
    "vae_runid = \"ad896cebfe4544e69e946d4f1bdc24aa\"\n",
    "checkpoint_path = None #\"/workspace/mlruns/4/7f18a82d78a84cd58f7947bb4e3986ff/artifacts/17_Step-198210.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:891: UserWarning: xebikart.vae is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# define the environment parameters\n",
    "# Load VAE\n",
    "vae = mlflow.keras.load_model(f\"runs:/{vae_runid}/model\", compile=False)\n",
    "env_params = GymVectorEnvironment(level='xebikart.gym.envs:create_env')\n",
    "env_params.additional_simulator_parameters = {\n",
    "    'level': 4, \n",
    "    'frame_skip': 2, \n",
    "    'max_cte_error': 6.0, \n",
    "    'min_steering': -1,\n",
    "    'max_steering': 1,\n",
    "    'min_throttle': 0.2, \n",
    "    'max_throttle': 0.4,\n",
    "    'vae': vae, \n",
    "    'n_history': n_history, \n",
    "    'max_steering_diff': 0.15, \n",
    "    'jerk_penalty_weight': 0.}\n",
    "\n",
    "# Soft Actor Critic\n",
    "agent_params = SoftActorCriticAgentParameters()\n",
    "agent_params.algorithm.num_consecutive_training_steps = num_training_per_episode\n",
    "agent_params.algorithm.act_for_full_episodes = True\n",
    "agent_params.algorithm.heatup_using_network_decisions = checkpoint_path is not None\n",
    "# exploration schedules\n",
    "agent_params.exploration.noise_schedule = LinearSchedule(0.1, 0., improve_steps)\n",
    "agent_params.exploration.evaluation_noise = 0.\n",
    "\n",
    "\n",
    "# visualize paremeters\n",
    "vis_params = VisualizationParameters()\n",
    "vis_params.print_networks_summary = True\n",
    "vis_params.dump_parameters_documentation = True\n",
    "vis_params.dump_mp4 = True\n",
    "# Default rules, dump at evaluation phase when a new total reward has been achieved \n",
    "# Uncomment to dump all video during evaluation phase\n",
    "#vis_params.video_dump_filters = [SelectedPhaseOnlyDumpFilter(RunPhase.TEST)]\n",
    "\n",
    "# schedule\n",
    "schedule_params = SimpleSchedule()\n",
    "schedule_params.heatup_steps = EnvironmentSteps(heatup_steps)\n",
    "schedule_params.improve_steps = TrainingSteps(improve_steps)\n",
    "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(steps_between_evaluation_periods)\n",
    "schedule_params.evaluation_steps = EnvironmentSteps(evaluation_steps)\n",
    "\n",
    "graph_manager = BasicRLGraphManager(\n",
    "    agent_params=agent_params,\n",
    "    env_params=env_params,\n",
    "    vis_params=vis_params,\n",
    "    schedule_params=schedule_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory\n",
    "experiment_tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46mCreating graph - name: BasicRLGraphManager\u001b[0m\n",
      "Looking in donkey_sim_home path: /sim/\n",
      "Looking for file: /sim/DonkeySimLinux/donkey_sim.x86_64\n",
      "Found: /sim/DonkeySimLinux/donkey_sim.x86_64\n",
      "Starting DonkeyGym env\n",
      "Donkey subprocess started\n",
      "Binding to ('0.0.0.0', 9091)\n",
      "Waiting for sim to start...if the simulation is running, press EXIT to go back to the menu\n",
      "Waiting for sim to start...if the simulation is running, press EXIT to go back to the menu\n",
      "Waiting for sim to start...if the simulation is running, press EXIT to go back to the menu\n",
      "Waiting for sim to start...if the simulation is running, press EXIT to go back to the menu\n",
      "Waiting for sim to start...if the simulation is running, press EXIT to go back to the menu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFailed to instantiate Gym environment class <function create_env at 0x7f5fc01762f0> with arguments {'level': 4, 'frame_skip': 2, 'max_cte_error': 6.0, 'min_steering': -1, 'max_steering': 1, 'min_throttle': 0.2, 'max_throttle': 0.4, 'vae': <tensorflow.python.keras.engine.training.Model object at 0x7f5f48b5e278>, 'n_history': 30, 'max_steering_diff': 0.15, 'jerk_penalty_weight': 0.0}\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-949020e5520b>\", line 12, in <module>\n",
      "    graph_manager.create_graph(task_params)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\", line 148, in create_graph\n",
      "    self.level_managers, self.environments = self._create_graph(task_parameters)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/rl_coach/graph_managers/basic_rl_graph_manager.py\", line 64, in _create_graph\n",
      "    visualization_parameters=self.visualization_parameters)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/rl_coach/environments/gym_environment.py\", line 279, in __init__\n",
      "    self.env = env_class(**self.additional_simulator_parameters)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/xebikart/gym/envs/__init__.py\", line 16, in create_env\n",
      "    min_throttle=min_throttle, max_throttle=max_throttle\n",
      "  File \"/usr/local/lib/python3.6/site-packages/xebikart/gym/envs/donkey_env.py\", line 85, in __init__\n",
      "    self.viewer.wait_until_loaded()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/xebikart/gym/core/donkey_sim.py\", line 53, in wait_until_loaded\n",
      "    time.sleep(3.0)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# create graph\n",
    "task_params = TaskParameters()\n",
    "task_params.num_gpu = 0\n",
    "task_params.use_cpu = True\n",
    "task_params.experiment_path = experiment_tempdir\n",
    "task_params.checkpoint_save_dir = experiment_tempdir\n",
    "# 30 min\n",
    "task_params.checkpoint_save_secs = 60 * 30\n",
    "# Use to start experiment from a checkpoint\n",
    "task_params.checkpoint_restore_path = checkpoint_path\n",
    "\n",
    "graph_manager.create_graph(task_params)\n",
    "\n",
    "logger.experiment_path = graph_manager.graph_logger.experiments_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"soft_actor_critic\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"improve_steps\", improve_steps)\n",
    "    mlflow.log_param(\"heatup_steps\", heatup_steps)\n",
    "    mlflow.log_param(\"evaluation_steps\", evaluation_steps)\n",
    "    mlflow.log_param(\"steps_between_evaluation_periods\", steps_between_evaluation_periods)\n",
    "    mlflow.log_param(\"num_training_per_episode\", num_training_per_episode)\n",
    "    mlflow.log_param(\"vae_runid\", vae_runid)\n",
    "    mlflow.log_param(\"n_history\", n_history)\n",
    "    mlflow.log_param(\"checkpoint\", checkpoint_path)\n",
    "    # TODO: think about saving the graph after improve\n",
    "    # TODO: maybe not needed\n",
    "    graph_manager.save_graph()\n",
    "    graph_manager.improve()\n",
    "    graph_manager.save_checkpoint()\n",
    "    mlflow.log_artifacts(experiment_tempdir)\n",
    "    # logs metrics\n",
    "    agent_metrics = pd.read_csv(f\"{experiment_tempdir}/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\")\n",
    "    mlflow.log_metric(\"episode_length_max\", agent_metrics[\"Episode Length\"].max())\n",
    "    mlflow.log_metric(\"episode_length_mean\", agent_metrics[\"Episode Length\"].mean())\n",
    "    mlflow.log_metric(\"episode_reward_max\", agent_metrics[\"Training Reward\"].max())\n",
    "    mlflow.log_metric(\"episode_reward_mean\", agent_metrics[\"Training Reward\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Visualize Reward Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "list_file = os.listdir(logger.experiment_path)\n",
    "csvfile = re.findall(pattern='[\\w.]*.csv', string=' '.join(list_file))\n",
    "df = pd.read_csv(os.path.join(logger.experiment_path, csvfile[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "plt.plot(df['Shaped Training Reward'])\n",
    "plt.title('Reward Evolution')\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode #</th>\n",
       "      <th>Training Iter</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>In Heatup</th>\n",
       "      <th>ER #Transitions</th>\n",
       "      <th>ER #Episodes</th>\n",
       "      <th>Episode Length</th>\n",
       "      <th>Total steps</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Shaped Training Reward</th>\n",
       "      <th>...</th>\n",
       "      <th>V_tgt_ns/Max</th>\n",
       "      <th>V_tgt_ns/Min</th>\n",
       "      <th>V_onl_ys/Mean</th>\n",
       "      <th>V_onl_ys/Stdev</th>\n",
       "      <th>V_onl_ys/Max</th>\n",
       "      <th>V_onl_ys/Min</th>\n",
       "      <th>actions/Mean</th>\n",
       "      <th>actions/Stdev</th>\n",
       "      <th>actions/Max</th>\n",
       "      <th>actions/Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>106.746385</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.151595</td>\n",
       "      <td>0.581465</td>\n",
       "      <td>0.958505</td>\n",
       "      <td>-0.996347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episode #  Training Iter  Epoch  In Heatup  ER #Transitions  ER #Episodes  \\\n",
       "0          1            0.0    0.0        1.0             83.0          83.0   \n",
       "1          2            0.0    0.0        1.0            176.0         176.0   \n",
       "2          3            0.0    0.0        1.0            284.0         284.0   \n",
       "3          4            0.0    0.0        1.0            388.0         388.0   \n",
       "4          5            0.0    0.0        0.0            502.0         502.0   \n",
       "\n",
       "   Episode Length  Total steps  Epsilon  Shaped Training Reward  ...  \\\n",
       "0            83.0         83.0      0.1                     NaN  ...   \n",
       "1            93.0        176.0      0.1                     NaN  ...   \n",
       "2           108.0        284.0      0.1                     NaN  ...   \n",
       "3           104.0        388.0      0.1                     NaN  ...   \n",
       "4           114.0        502.0      0.1              106.746385  ...   \n",
       "\n",
       "   V_tgt_ns/Max  V_tgt_ns/Min  V_onl_ys/Mean  V_onl_ys/Stdev  V_onl_ys/Max  \\\n",
       "0           NaN           NaN            NaN             NaN           NaN   \n",
       "1           NaN           NaN            NaN             NaN           NaN   \n",
       "2           NaN           NaN            NaN             NaN           NaN   \n",
       "3           NaN           NaN            NaN             NaN           NaN   \n",
       "4           NaN           NaN            NaN             NaN           NaN   \n",
       "\n",
       "   V_onl_ys/Min  actions/Mean  actions/Stdev  actions/Max  actions/Min  \n",
       "0           NaN           NaN            NaN          NaN          NaN  \n",
       "1           NaN           NaN            NaN          NaN          NaN  \n",
       "2           NaN           NaN            NaN          NaN          NaN  \n",
       "3           NaN           NaN            NaN          NaN          NaN  \n",
       "4           NaN     -0.151595       0.581465     0.958505    -0.996347  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing donkey sim subprocess\n",
      "Server shutdown\n",
      "Connection dropped\n"
     ]
    }
   ],
   "source": [
    "graph_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
