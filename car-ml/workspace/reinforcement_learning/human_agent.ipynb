{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use human_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import SimpleSchedule\n",
    "from rl_coach.base_parameters import TaskParameters, VisualizationParameters\n",
    "from rl_coach.core_types import EnvironmentSteps, TrainingSteps, SelectedPhaseOnlyDumpFilter, RunPhase\n",
    "from rl_coach.schedules import LinearSchedule\n",
    "\n",
    "from rl_coach import logger\n",
    "\n",
    "from xebikart.gym.envs import rewards as gym_rewards\n",
    "from xebikart.agent import XebikartHumanAgentParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "improve_steps = 400000\n",
    "max_cte_error = 5.0 # max space between the car and the center of the road before ending an episode\n",
    "throttle = 0.20\n",
    "scale_cte_reward=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DONKEY_SIM_HOME\"] = \"/Users/nlaille/UnityProjects/xebikart-unity/outputs\"\n",
    "vae_path = \"vae_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_cte(cte, scale):\n",
    "    return (0.25*scale)-((((cte / max_cte_error) - 0.5)**2)*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom reward\n",
    "def build_reward_fn(cte_scale_reward_weight, crash_reward_weight):\n",
    "    def _custom_reward_fn(reward, done, info):\n",
    "        \"\"\"\n",
    "        Custom reward function\n",
    "        \n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :param info:\n",
    "            \"x\": \n",
    "            \"y\": \n",
    "            \"z\": \n",
    "            \"speed\": \n",
    "            \"cte\": \n",
    "            \"hit\": \n",
    "            \"throttle\": \n",
    "            \"steering\": \n",
    "        \"\"\"\n",
    "        \n",
    "        if done:\n",
    "            # penalize the agent for getting off the road fast\n",
    "            return crash_reward_weight\n",
    "        else:\n",
    "            cte = info[\"cte\"]\n",
    "            return reward_cte(cte, cte_scale_reward_weight)\n",
    "    return _custom_reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment parameters\n",
    "# Load VAE\n",
    "vae = tf.keras.models.load_model(vae_path)\n",
    "env_params = GymVectorEnvironment(level='xebikart.gym.envs:create_fix_throttle_env')\n",
    "env_params.human_control = True\n",
    "env_params.additional_simulator_parameters = {\n",
    "  'throttle': throttle, 'vae': vae, 'max_cte_error': max_cte_error, \n",
    "  'reward_fn': build_reward_fn(cte_scale_reward_weight=scale_cte_reward, crash_reward_weight=-20)\n",
    "}\n",
    "\n",
    "# Human agent\n",
    "agent_params = XebikartHumanAgentParameters()\n",
    "\n",
    "# schedule\n",
    "schedule_params = SimpleSchedule()\n",
    "schedule_params.heatup_steps = EnvironmentSteps(0)\n",
    "schedule_params.improve_steps = TrainingSteps(improve_steps)\n",
    "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(improve_steps)\n",
    "schedule_params.evaluation_steps = EnvironmentSteps(0)\n",
    "\n",
    "graph_manager = BasicRLGraphManager(\n",
    "    agent_params=agent_params,\n",
    "    env_params=env_params,\n",
    "    schedule_params=schedule_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory\n",
    "experiment_tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "task_params = TaskParameters()\n",
    "task_params.num_gpu = 0\n",
    "task_params.use_cpu = True\n",
    "task_params.experiment_path = experiment_tempdir\n",
    "\n",
    "graph_manager.create_graph(task_params)\n",
    "graph_manager.improve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_manager.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
