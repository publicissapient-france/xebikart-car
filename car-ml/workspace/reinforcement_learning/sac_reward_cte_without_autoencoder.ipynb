{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic - Reward based on CTE\n",
    "\n",
    "Train a soft actor critic agent based on rl_coach framework : https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html\n",
    "\n",
    "Some \"filters\" are applied on observation before being supply to soft actor critic policies :\n",
    "- Convert tensor uint8 type into float32\n",
    "- Convert rgb images to grayscale\n",
    "- Reshape image by cropping from (120, 160) -> (80, 160)\n",
    "- Apply sobel filter (https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "  - Binarize images by setting elements to 0 or 1\n",
    "\n",
    "## Reward function\n",
    "\n",
    "Scale reward from 0 to 5 (5 being center of the track)\n",
    "\n",
    "-200 penalty on crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking.artifact_utils import _download_artifact_from_uri\n",
    "\n",
    "from rl_coach.agents.soft_actor_critic_agent import SoftActorCriticAgentParameters\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import SimpleSchedule\n",
    "from rl_coach.base_parameters import TaskParameters, VisualizationParameters\n",
    "from rl_coach.core_types import EnvironmentSteps, TrainingSteps, SelectedPhaseOnlyDumpFilter, RunPhase\n",
    "from rl_coach.schedules import LinearSchedule\n",
    "\n",
    "from rl_coach import logger\n",
    "\n",
    "from xebikart.gym.envs import rewards as gym_rewards\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "improve_steps = 1200000\n",
    "heatup_steps = 300\n",
    "evaluation_steps = 800\n",
    "steps_between_evaluation_periods = 300000\n",
    "num_training_per_episode = 50\n",
    "checkpoint_runid = None# \"3e46980d55fd457f9afdc56be5636d36\"\n",
    "checkpoint_id = None#\"18_Step-200000.ckpt\"\n",
    "max_cte_error = 6.0 # max space between the car and the center of the road before ending an episode\n",
    "throttle = 0.30\n",
    "scale_cte_reward=5\n",
    "crash_reward=-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoint from mlflow\n",
    "def download_ckpt(run_id, ckpt_id):\n",
    "    mlclient = mlflow.tracking.MlflowClient()\n",
    "    artifacts = mlclient.list_artifacts(run_id)\n",
    "    ckpt_files = list(filter(lambda x: ckpt_id in x.path, artifacts))\n",
    "    if len(ckpt_files) == 0:\n",
    "        raise RuntimeError(f\"No checkpoint found for run {run_id} and checkpoint {ckpt_id}\")\n",
    "    output = tempfile.mkdtemp()\n",
    "    for ckpt_file in ckpt_files:\n",
    "        print(f\"runs:/{run_id}/{ckpt_file.path}\")\n",
    "        _download_artifact_from_uri(f\"runs:/{run_id}/{ckpt_file.path}\", output_path=output)\n",
    "    return f\"{output}/{ckpt_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_runid is not None and checkpoint_id is not None:\n",
    "    checkpoint_path = download_ckpt(checkpoint_runid, checkpoint_id)\n",
    "else:\n",
    "    checkpoint_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward distribution based on cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_cte(cte, scale):\n",
    "    return (scale*-(math.fabs(cte) / max_cte_error) ** 2) + scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = np.linspace(-max_cte_error, max_cte_error, 100)\n",
    "rew = [reward_cte(i, scale_cte_reward) for i in ran]\n",
    "\n",
    "plt.plot(ran, rew)\n",
    "plt.plot((0,0),(-1,scale_cte_reward),'--')\n",
    "plt.plot((-max_cte_error,-max_cte_error),(-1,scale_cte_reward),'-',c=\"green\")\n",
    "plt.plot((max_cte_error,max_cte_error),(-1,scale_cte_reward),'-',c=\"green\")\n",
    "plt.title(\"Reward distribution based on cte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom reward\n",
    "def build_reward_fn(cte_scale_reward_weight, crash_reward_weight):\n",
    "    def _custom_reward_fn(reward, done, info):\n",
    "        \"\"\"\n",
    "        Custom reward function\n",
    "        \n",
    "        :param reward:\n",
    "        :param done:\n",
    "        :param info:\n",
    "            \"x\": \n",
    "            \"y\": \n",
    "            \"z\": \n",
    "            \"speed\": \n",
    "            \"cte\": \n",
    "            \"hit\": \n",
    "            \"throttle\": \n",
    "            \"steering\": \n",
    "        \"\"\"\n",
    "        \n",
    "        if done:\n",
    "            # penalize the agent for getting off the road fast\n",
    "            return crash_reward_weight\n",
    "        else:\n",
    "            cte = info[\"cte\"]\n",
    "            return reward_cte(cte, cte_scale_reward_weight)\n",
    "    return _custom_reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment parameters\n",
    "env_params = GymVectorEnvironment(level='xebikart.gym.envs:create_fix_throttle_env')\n",
    "env_params.additional_simulator_parameters = {\n",
    "  'throttle': throttle, 'max_cte_error': max_cte_error, \n",
    "  'reward_fn': build_reward_fn(cte_scale_reward_weight=scale_cte_reward, crash_reward_weight=crash_reward)\n",
    "}\n",
    "\n",
    "# Soft Actor Critic\n",
    "agent_params = SoftActorCriticAgentParameters()\n",
    "agent_params.algorithm.num_consecutive_training_steps = num_training_per_episode\n",
    "agent_params.algorithm.act_for_full_episodes = True\n",
    "agent_params.algorithm.heatup_using_network_decisions = checkpoint_path is not None\n",
    "# exploration schedules\n",
    "agent_params.exploration.noise_schedule = LinearSchedule(0.3, 0., improve_steps)\n",
    "agent_params.exploration.evaluation_noise = 0.\n",
    "\n",
    "\n",
    "# visualize paremeters\n",
    "vis_params = VisualizationParameters()\n",
    "vis_params.print_networks_summary = True\n",
    "vis_params.dump_parameters_documentation = True\n",
    "vis_params.dump_mp4 = True\n",
    "# Default rules, dump at evaluation phase when a new total reward has been achieved \n",
    "# Uncomment to dump all video during evaluation phase\n",
    "#vis_params.video_dump_filters = [SelectedPhaseOnlyDumpFilter(RunPhase.TRAIN)]\n",
    "\n",
    "# schedule\n",
    "schedule_params = SimpleSchedule()\n",
    "schedule_params.heatup_steps = EnvironmentSteps(heatup_steps)\n",
    "schedule_params.improve_steps = TrainingSteps(improve_steps)\n",
    "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(steps_between_evaluation_periods)\n",
    "schedule_params.evaluation_steps = EnvironmentSteps(evaluation_steps)\n",
    "\n",
    "graph_manager = BasicRLGraphManager(\n",
    "    agent_params=agent_params,\n",
    "    env_params=env_params,\n",
    "    vis_params=vis_params,\n",
    "    schedule_params=schedule_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory\n",
    "experiment_tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "task_params = TaskParameters()\n",
    "task_params.num_gpu = 0\n",
    "task_params.use_cpu = True\n",
    "task_params.experiment_path = experiment_tempdir\n",
    "task_params.checkpoint_save_dir = experiment_tempdir\n",
    "# 30 min\n",
    "task_params.checkpoint_save_secs = 6 * 60 * 30\n",
    "# Use to start experiment from a checkpoint\n",
    "task_params.checkpoint_restore_path = checkpoint_path\n",
    "\n",
    "graph_manager.create_graph(task_params)\n",
    "\n",
    "logger.experiment_path = graph_manager.graph_logger.experiments_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"rl_reward_cte\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"improve_steps\", improve_steps)\n",
    "    mlflow.log_param(\"heatup_steps\", heatup_steps)\n",
    "    mlflow.log_param(\"evaluation_steps\", evaluation_steps)\n",
    "    mlflow.log_param(\"steps_between_evaluation_periods\", steps_between_evaluation_periods)\n",
    "    mlflow.log_param(\"num_training_per_episode\", num_training_per_episode)\n",
    "    mlflow.log_param(\"throttle\", throttle)\n",
    "    mlflow.log_param(\"checkpoint_runid\", checkpoint_runid)\n",
    "    mlflow.log_param(\"checkpoint_id\", checkpoint_id)\n",
    "    # TODO: think about saving the graph after improve\n",
    "    # TODO: maybe not needed\n",
    "    graph_manager.save_graph()\n",
    "    graph_manager.improve()\n",
    "    graph_manager.save_checkpoint()\n",
    "    mlflow.log_artifacts(experiment_tempdir)\n",
    "    # logs metrics\n",
    "    agent_metrics = pd.read_csv(f\"{experiment_tempdir}/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\")\n",
    "    mlflow.log_metric(\"episode_length_max\", agent_metrics[\"Episode Length\"].max())\n",
    "    mlflow.log_metric(\"episode_length_mean\", agent_metrics[\"Episode Length\"].mean())\n",
    "    mlflow.log_metric(\"episode_reward_max\", agent_metrics[\"Training Reward\"].max())\n",
    "    mlflow.log_metric(\"episode_reward_mean\", agent_metrics[\"Training Reward\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_manager.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
