{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model to detect images with an obstacle on track and make a lite version of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : Johan Jublanc\n",
    "    \n",
    "Date : 22/09/2019\n",
    "\n",
    "Description : \n",
    "\n",
    "Use the Python interpreter to load a .tflite file and run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO :_\n",
    "* Generalize this procedure to each model in the car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from xebikart.images import transformer as T\n",
    "\n",
    "import xebikart.dataset as dataset\n",
    "import xebikart.lite_functions as lf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.compat.v1 import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and convert the keras model (.h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model keras (.h5) is loaded and convert to a lite model (.tflite) which is lighter :\n",
    "* a smaller file thank to the serialization library FlatBuffer\n",
    "* a model smaller by reducing the precision of the numbers in the model\n",
    "\n",
    "See : \n",
    "* https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "* https://www.tensorflow.org/lite/convert/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a converter using the keras model as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lite output model\n",
    "model_name = \"models/drive_auto_int\"\n",
    "\n",
    "# h5 intput model\n",
    "run_path = \"../mlruns/9/\"\n",
    "model_id = \"ac9ca9900320450db0fc8ec3431a7859\"\n",
    "h5_path = \"/artifacts/model/data/model.h5\"\n",
    "model_path = run_path + model_id + h5_path\n",
    "#mlruns/8/c1a25f0c583d4d1a943e4ab270747aef/artifacts/model/data/model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    converter = lite.TFLiteConverter.from_keras_model_file(model_path)\n",
    "except:\n",
    "    print(\"No model named {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parametrize an optimizer to quantize the model so that it is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the lite version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"{}.tflite\".format(model_name), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the inference process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tflite models, inferences are made thanks to an __interpreter__ that is fast and lean. We will define some functions to handle the whole prediction process with the Python API : \n",
    "* first, prepare the image -> read/normalize/resize/reshape/convert\n",
    "* second, define an interpreter and input and ouput details\n",
    "* finally, define a predictor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fn = T.auto_drive_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter, input_details, output_details = lf.interpreter_and_details(model_name + \".tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = lf.predictor_builder(model_path = model_name + \".tflite\", preprocess_fn = preprocess_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the inference process on a random image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "tubes_root_folder = \"file:/workspace/xebikart-ml-tubes\"\n",
    "tubes_folders = [\n",
    "    \"tub.v4.03\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tubes_df = dataset.get_tubes_df(tubes_root_folder, tubes_folders, tubes_extension=\".tar.gz\")\n",
    "tubes_df = raw_tubes_df.rename(columns={\"cam/image_array\": \"images_path\", \"user/angle\": \"angles\", \"user/throttle\": \"throttles\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = []\n",
    "for i in range(4):\n",
    "    random_image_path.append(tubes_df.sample()[\"images_path\"].values[0])\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15,15), constrained_layout=True)\n",
    "\n",
    "for i in range(4):\n",
    "    # function pre-defined are used to compute the prediction\n",
    "    tf_image = T.read_image(random_image_path[i])\n",
    "    prediction = predictor(tf_image)\n",
    "    \n",
    "    # eaxh image in shown with the prediction\n",
    "    axs[i].set_title(\"Prediction = {}\".format(prediction))\n",
    "    axs[i].imshow(tf_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
